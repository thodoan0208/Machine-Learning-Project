{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1aa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889219c",
   "metadata": {},
   "source": [
    "## 1[a].Get  your first DT Classifier runing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf040f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9103bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.keys(): dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "Shape of cancer data: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"data.keys(): {}\".format(data.keys()))\n",
    "print(\"Shape of cancer data: {}\".format(data.data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfca7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts per class:\n",
      "{'malignant': 212, 'benign': 357}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample counts per class:\\n{}\".format({n: v for n, v in zip(data.target_names, np.bincount(data.target))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c609af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\\n{}\".format(data.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77395131",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95422a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, stratify=data.target, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38f7804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c48791",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=0, splitter='best')\n",
    "\n",
    "Now that we fit the model to the training data, the next step would be to figure how well did the model perform. This can be done by calling the .score method. This is display the mean accuracy on given test data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b0128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.937\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on training set: {:.3f}'.format(tree.score(X_train, y_train)))\n",
    "print('Accuracy on test set: {:.3f}'.format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e70863",
   "metadata": {},
   "source": [
    " From the above analysis involving different depths of the decision tree, we can see that a depth of 3 is most optimal. This is due to the fact that it has a good accuracy for training data (95.3%) (doesn't memorize test labels) and generalizes well to the test data (94.4%)\n",
    "\n",
    "Now that to have the optimal model figured, let's move on to visualize the decision tree we came up with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32580eb9",
   "metadata": {},
   "source": [
    "## 1[b]: Ploting - same example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59563a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe9e43",
   "metadata": {},
   "source": [
    "To read the file we exported above to create a visualization by using graphviz module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], feature_name=data.feature_names, impurity=False, filled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(model, out_file = \"model.dot\", class_names = [\"malignant\", \"benign\"], feature_names = canc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    " \n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf44bd",
   "metadata": {},
   "source": [
    "Our deicison tree is shown above, let's note down some inferences that we can draw from it below:\n",
    "\n",
    "- Right node of the tree (Depth 1, right): The node starts off with 129 samples, out of which 127 are malignant and 2 are benign.This node splits from the root since the worst perimeter for these samples > 115.35.\n",
    "- As we move further down that node, some finer distinctions are used to split off the 2 benign samples. All the malignant cases accumulate on the rightmost leaf. Here, the gini score is 0.0 which means this only applies to malignant class. A node's gini attribute measures it's impurity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47ca4a",
   "metadata": {},
   "source": [
    "- Left node of the tree (Depth 1, left): The node starts off with 32 malignant and 265 benign cases (worst perimeter <= 115.35.\n",
    "With each step, more distinctions are applied and we end up with most of the benign cases in the leftmost leaf where we have a gini score of 0.034."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d6e06",
   "metadata": {},
   "source": [
    " Visualizing Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e43623",
   "metadata": {},
   "source": [
    "export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe6755",
   "metadata": {},
   "source": [
    "In the decision tree chart, each internal node has a decision rule that splits the data. Gini referred as Gini ratio, which measures the impurity of the node. You can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node.\n",
    "\n",
    "Here, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let's optimize it by pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "print(\"The feature importances for different features are: \\n{}\".format(feature_importanc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee485b",
   "metadata": {},
   "source": [
    "The feature importances for different features are:\n",
    "\n",
    "[0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.00576963 0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.81041464 0.\n",
    " 0.         0.         0.02239144 0.1614243  0.         0.        ]\n",
    "\n",
    "These numbers don't make much sense, do they? Let's now plot the above results to make it more comprehensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = data_train.shape[1]\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), data_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980aadf",
   "metadata": {},
   "source": [
    "From the barplot above:\n",
    "\n",
    "- Worst perimeter is the most important feature. This is evident from the fact that this was the first condition that was applied for coming up with the decision tree.\n",
    "- Features with low feature importances don't mean that they are not important, rather they were not picked by the tree which can occur due to the fact that different variables weren't independent of each other.\n",
    "- Feature importances tell us that worst perimeter is important but not about whether a sample is benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50dc19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
